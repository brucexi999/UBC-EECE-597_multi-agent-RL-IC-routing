{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import heapq\n",
    "import ray\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "from ray import tune, air\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.utils.pre_checks.env import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the custom routing grid environment\n",
    "\n",
    "# action space encoding\n",
    "    # 0 - > move up\n",
    "    # 1 - > move down\n",
    "    # 2 - > move right\n",
    "    # 3 - > move left\n",
    "# state (observation) space encoding\n",
    "    # state[0:1] = current agent position\n",
    "    # state[2:3] = current goal position\n",
    "    # state[4:7] = capacities of the four neighboring edges of the current agent position. Namely, up, right, down, and left\n",
    "\n",
    "\n",
    "class RtGridEnv(MultiAgentEnv):\n",
    "    def __init__(self, length:int, width:int, nets:list, macros:list, edge_capacity:np.ndarray, max_step:int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            length (int): length of the canvas\n",
    "            width (int): width of the canvas\n",
    "            nets (list): a list of nets to be routed\n",
    "            macros (list): a list of macros that has been placed on the canvas by placement\n",
    "        \"\"\"\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "        self.nets = nets\n",
    "        self.n_nets = len(self.nets)\n",
    "        self.macros = macros\n",
    "        self.initial_capacity = edge_capacity.copy()\n",
    "        self.initial_capacity.setflags(write=False)\n",
    "        self.edge_capacity = edge_capacity.copy()\n",
    "        self.max_capacity = np.max(self.edge_capacity) + 1 # plus one to account for the behavior of gym.MultiDiscrete\n",
    "        self.max_step = max_step\n",
    "        self.step_counter = 0 # counts the number of steps elapsed for the current episode\n",
    "        \n",
    "        self.agents_id = []\n",
    "        for i in range(self.n_nets):\n",
    "            self.agents_id.append(\"agent_{}\".format(i))\n",
    "        self.state = {}\n",
    "        self.agent_position = {}\n",
    "        self.goal_position = {}\n",
    "        self.change_pin_flag = self.reset_flags({})\n",
    "        self.done_flag = self.reset_flags({})\n",
    "        #print(\"change pin flags \", self.change_pin_flag)\n",
    "        # the done flag needs an additional \"__all__\" key to indicate all agents are done\n",
    "        self.done_flag[\"__all__\"] = False\n",
    "        #print(\"done flags \", self.done_flag)\n",
    "        self.pin_counter = {}\n",
    "        self.reset_pin_counters()\n",
    "        self.path_x = self.generate_path(self.nets)\n",
    "        self.path_y = self.generate_path(self.nets)\n",
    "        #print(self.path_x)\n",
    "        self.decomposed_nets = {}\n",
    "        for i in range(self.n_nets): \n",
    "            self.decomposed_nets[self.agents_id[i]] = self.prim_mst(self.nets[i])\n",
    "        \n",
    "        # initialize the agent to route the first 2-pin net decomposed from the first multi-pin net\n",
    "        for agent_id in self.agents_id:\n",
    "            self.update_positions(agent_id)\n",
    "            self.update_path(agent_id)\n",
    "        #print(self.agent_position)\n",
    "        #print(self.goal_position)\n",
    "        # initialize the path lists\n",
    "        #for agent_id in self.agents_id:\n",
    "            #self.update_path(agent_id)\n",
    "        #print(self.path_x)\n",
    "        #print(self.path_y)\n",
    "        # define the action and the observation space\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = MultiDiscrete(\n",
    "            [\n",
    "                self.length, \n",
    "                self.width, \n",
    "                self.length, \n",
    "                self.width, \n",
    "                self.max_capacity, \n",
    "                self.max_capacity, \n",
    "                self.max_capacity, \n",
    "                self.max_capacity\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def update_positions(self, agent_id:str):\n",
    "        \"\"\"\n",
    "        Update the agent position with the starting pin of the next 2-pin net.\n",
    "        Update the goal position with the new goal.\n",
    "        \"\"\"\n",
    "        self.agent_position[agent_id] = np.array(self.decomposed_nets[agent_id]['u'][self.pin_counter[agent_id]])\n",
    "        self.goal_position[agent_id] = np.array(self.decomposed_nets[agent_id]['v'][self.pin_counter[agent_id]])\n",
    "\n",
    "    def update_path(self, agent_id:str):\n",
    "        \"\"\"Update the path agent has traveled.\"\"\"\n",
    "        self.path_x[agent_id][self.pin_counter[agent_id]].append(self.agent_position[agent_id][0])\n",
    "        self.path_y[agent_id][self.pin_counter[agent_id]].append(self.agent_position[agent_id][1])\n",
    "\n",
    "    def reset_pin_counters(self):\n",
    "        \"\"\"Set the pin counter of each agent to 0.\"\"\"\n",
    "        for i in range(self.n_nets):\n",
    "            self.pin_counter[self.agents_id[i]] = 0\n",
    "    \n",
    "    def reset_flags(self, flags:dict):\n",
    "        for i in range(self.n_nets):\n",
    "            flags[self.agents_id[i]] = False\n",
    "\n",
    "        return flags\n",
    "\n",
    "    def generate_path(self, nets:list):\n",
    "        \"\"\"Generate the list data structure to hold the path traveled by the agent.\"\"\"\n",
    "        path = {}\n",
    "        for i in range(len(nets)):\n",
    "            path[self.agents_id[i]] = []\n",
    "            for j in range(len(nets[i])-1):\n",
    "                path[self.agents_id[i]].append([])\n",
    "\n",
    "        return path\n",
    "\n",
    "    def prim_mst(self, pins):\n",
    "        \"\"\"\n",
    "        Compute the Minimum Spanning Tree (MST) using Prim's algorithm.\n",
    "\n",
    "        Args:\n",
    "            pins (list): List of (x, y) coordinates representing the pin locations.\n",
    "\n",
    "        Returns:\n",
    "            dict: a dictionary containing the vertices of all the edges in the MST\n",
    "\n",
    "        Note:\n",
    "            - The pins list should contain at least two points.\n",
    "        \"\"\"\n",
    "\n",
    "        def euclidean_distance(p1, p2):\n",
    "            \"\"\"\n",
    "            Compute the Euclidean distance between two points.\n",
    "\n",
    "            Args:\n",
    "                p1 (tuple): First point (x, y) coordinates.\n",
    "                p2 (tuple): Second point (x, y) coordinates.\n",
    "\n",
    "            Returns:\n",
    "                float: Euclidean distance between the two points.\n",
    "            \"\"\"\n",
    "            x1, y1 = p1\n",
    "            x2, y2 = p2\n",
    "            return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "        \n",
    "        distances = {}\n",
    "        for i in range(len(pins)):\n",
    "            for j in range(i+1, len(pins)):\n",
    "                p1 = pins[i]\n",
    "                p2 = pins[j]\n",
    "                distances[(i, j)] = euclidean_distance(p1, p2)\n",
    "                distances[(j, i)] = distances[(i, j)]  # Add symmetric distance\n",
    "        \n",
    "        # Initialize\n",
    "        num_pins = len(pins)\n",
    "        visited = [False] * num_pins\n",
    "        mst_u = []\n",
    "        mst_v = []\n",
    "        start_vertex = 0\n",
    "        visited[start_vertex] = True\n",
    "        \n",
    "        # Create a priority queue\n",
    "        pq = []\n",
    "        \n",
    "        # Mark the initial vertex as visited\n",
    "        for i in range(num_pins):\n",
    "            if i != start_vertex:\n",
    "                heapq.heappush(pq, (distances[(start_vertex, i)], start_vertex, i))\n",
    "        \n",
    "        # Update the priority queue and perform Prim's algorithm\n",
    "        while pq:\n",
    "            if (len(mst_u) == len(pins) -1): # for n pins, the MST should at most have n-1 edges\n",
    "                break\n",
    "\n",
    "            weight, u, v = heapq.heappop(pq)\n",
    "            \n",
    "            if visited[v]:\n",
    "                #print(f\"Skipping edge: {weight} - {u} - {v}\")\n",
    "                continue\n",
    "            \n",
    "            # Prim's algorithm iteration\n",
    "            visited[v] = True\n",
    "            mst_u.append(pins[u])\n",
    "            mst_v.append(pins[v])\n",
    "            \n",
    "            for i in range(num_pins):\n",
    "                if not visited[i]:\n",
    "                    heapq.heappush(pq, (distances[(v, i)], v, i))\n",
    "        \n",
    "        mst = {'u':mst_u,'v':mst_v}\n",
    "\n",
    "        return mst\n",
    "    \n",
    "    def update_capacity(self, agent_position:np.ndarray, action:int):\n",
    "        \"\"\"\n",
    "        Update the edge capacities after taking an action\n",
    "        \"\"\"\n",
    "\n",
    "        # reduce the capacity of the current node\n",
    "        self.edge_capacity[agent_position[0]][agent_position[1]][action] += -1\n",
    "\n",
    "        # reduce the capacity of the next node's corresponding edge\n",
    "        new_node = self.compute_new_position(agent_position, action)\n",
    "        corresponding_edge = (action + 2) % 4\n",
    "        self.edge_capacity[new_node[0]][new_node[1]][corresponding_edge] += -1\n",
    "        \n",
    "    def compute_new_position(self, agent_position:np.ndarray, action:int):\n",
    "        \"\"\"\n",
    "        Compute new agent positions\n",
    "        \"\"\"\n",
    "        if action == 0: # up\n",
    "            new_position = (agent_position[0], agent_position[1]+1)\n",
    "        elif action == 1: # right\n",
    "            new_position = (agent_position[0]+1, agent_position[1])\n",
    "        elif action == 2: # down\n",
    "            new_position = (agent_position[0], agent_position[1]-1)\n",
    "        elif action == 3: # left\n",
    "            new_position = (agent_position[0]-1, agent_position[1])\n",
    "        \n",
    "        return new_position\n",
    "    \n",
    "    def step(self, action:dict):\n",
    "        def check_move_validity(agent_position:np.ndarray, action:int):\n",
    "            \"\"\"\n",
    "            Check whether a move is valid by checking:\n",
    "            (1) the edge the move is about to use has capacity greater than 0\n",
    "            (2) the position after the move is not within macro regions\n",
    "            (3) the position after the move in within in the routing canvas\n",
    "            Returns True if the move is valid, False if invalid\n",
    "            \"\"\"\n",
    "            # capacity of the 4 neighboring edges of the current agent position\n",
    "            node_capacity = self.edge_capacity[agent_position[0]][agent_position[1]]\n",
    "\n",
    "            new_position = self.compute_new_position(agent_position, action)\n",
    "\n",
    "            macro_flag = new_position not in self.macros\n",
    "            bound_flag = new_position[0] in range(self.length) and new_position[1] in range(self.width)\n",
    "            capacity_flag = node_capacity[action] > 0\n",
    "\n",
    "            valid = macro_flag and bound_flag and capacity_flag\n",
    "\n",
    "            return valid\n",
    "        \n",
    "        # extract all the active agents in this time step\n",
    "        active_agent = list(action.keys())\n",
    "        #print(\"active agent \", active_agent)\n",
    "        reward = {}\n",
    "\n",
    "        # if we have reached our maximum time step, set the all done flag\n",
    "        self.step_counter += 1\n",
    "        if self.step_counter >= self.max_step:\n",
    "            for agent_id in active_agent:\n",
    "                reward[agent_id] = -1\n",
    "            self.done_flag[\"__all__\"] = True\n",
    "            #print(\"reward \", reward)\n",
    "            #print(\"done flags \", self.done_flag)\n",
    "            return self.state, reward, self.done_flag, {}\n",
    "        \n",
    "        # update pins for those agents in need\n",
    "        # Only agents that have the change pin flag set to True AND are active, will be updated\n",
    "        pin_flag_agents = [key for key, value in self.change_pin_flag.items() if value]\n",
    "        pin_flag_agents = list(set(active_agent).intersection(set(pin_flag_agents)))\n",
    "        for agent_id in pin_flag_agents:\n",
    "            #print(agent_id)\n",
    "            self.update_positions(agent_id)\n",
    "            self.update_path(agent_id)\n",
    "            reward[agent_id] = 0\n",
    "            self.change_pin_flag[agent_id] = False\n",
    "            self.state[agent_id] = np.concatenate([\n",
    "                self.agent_position[agent_id], \n",
    "                self.goal_position[agent_id], \n",
    "                self.edge_capacity[self.agent_position[agent_id][0]][self.agent_position[agent_id][1]]\n",
    "                ])\n",
    "            active_agent.remove(agent_id) # de-active agents that undergoes pin-upgrading, such that they won't be unintentionally accessed\n",
    "        #TODO: COMPUTE NEW STATE AND REWARD FOR ACTIVE AGENTS\n",
    "        for agent_id in active_agent:\n",
    "            if check_move_validity(self.agent_position[agent_id], action[agent_id]):\n",
    "                self.update_capacity(self.agent_position[agent_id], action[agent_id])\n",
    "                self.agent_position[agent_id] = np.array(list(self.compute_new_position(self.agent_position[agent_id], action[agent_id])))\n",
    "                self.update_path(agent_id)\n",
    "\n",
    "            if np.array_equal(self.agent_position[agent_id], self.goal_position[agent_id]):\n",
    "                reward[agent_id] = 1000\n",
    "                self.update_counters(agent_id)\n",
    "            else:\n",
    "                reward[agent_id] = -1\n",
    "            \n",
    "            self.state[agent_id] = np.concatenate([\n",
    "                self.agent_position[agent_id], \n",
    "                self.goal_position[agent_id], \n",
    "                self.edge_capacity[self.agent_position[agent_id][0]][self.agent_position[agent_id][1]]\n",
    "                ])\n",
    "        # if all agents are done, set the __all__ flag\n",
    "        self.done_flag[\"__all__\"] = all(self.done_flag[agent_key] for agent_key in self.done_flag if agent_key.startswith('agent_'))\n",
    "        \n",
    "        return self.state, reward, self.done_flag, {}\n",
    "        #print(\"state \", self.state)\n",
    "        #print(\"change pin flag \", self.change_pin_flag)\n",
    "        #print(\"reward \", reward)\n",
    "\n",
    "    def update_counters(self, agent_id:str):\n",
    "        # one 2-pin net within one multi-pin net is done\n",
    "        self.pin_counter[agent_id] += 1\n",
    "        self.change_pin_flag[agent_id] = True\n",
    "        net_id = int(agent_id.split(\"_\")[1])\n",
    "\n",
    "        if self.pin_counter[agent_id] == len(self.nets[net_id]) - 1:\n",
    "            # this agent is done, it has routed all the pins\n",
    "            #self.pin_counter[agent_id] = 0\n",
    "            self.change_pin_flag[agent_id] = False\n",
    "            self.done_flag[agent_id] = True\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        #print(\"RESETTING...\")\n",
    "        self.reset_pin_counters()\n",
    "        #print(\"pin counters \", self.pin_counter)\n",
    "        self.step_counter = 0\n",
    "        self.change_pin_flag = self.reset_flags(self.change_pin_flag)\n",
    "        self.done_flag = self.reset_flags(self.done_flag)\n",
    "        self.done_flag[\"__all__\"] = False\n",
    "        #print(\"change pin flags \", self.change_pin_flag)\n",
    "        #print(\"done flags \", self.done_flag)\n",
    "        self.edge_capacity = self.initial_capacity.copy()\n",
    "        self.path_x = self.generate_path(self.nets)\n",
    "        self.path_y = self.generate_path(self.nets)\n",
    "        #print(\"path x \", self.path_x)\n",
    "        #print(\"path y \", self.path_y)\n",
    "        for agent_id in self.agents_id:\n",
    "            self.update_positions(agent_id)\n",
    "            self.update_path(agent_id)\n",
    "        #for agent_id in self.agents_id:\n",
    "            #self.update_path(agent_id)\n",
    "        #print(\"agent positions \", self.agent_position)\n",
    "        #print(\"goal positions \", self.goal_position)\n",
    "        #print(\"path x \", self.path_x)\n",
    "        #print(\"path y \", self.path_y)\n",
    "        for agent_id in self.agents_id:\n",
    "            individual_state = np.concatenate([\n",
    "                self.agent_position[agent_id], \n",
    "                self.goal_position[agent_id], \n",
    "                self.edge_capacity[self.agent_position[agent_id][0]][self.agent_position[agent_id][1]]\n",
    "                ])\n",
    "            self.state[agent_id] = individual_state\n",
    "        \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_0': array([2, 1, 2, 3, 6, 6, 6, 6]), 'agent_1': array([1, 4, 3, 4, 6, 6, 6, 6]), 'agent_2': array([1, 5, 1, 2, 6, 6, 6, 6]), 'agent_3': array([0, 5, 3, 5, 6, 6, 6, 6]), 'agent_4': array([3, 1, 4, 3, 6, 6, 6, 6]), 'agent_5': array([0, 0, 5, 0, 6, 6, 6, 6])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'env.step_counter = 999\\nenv.pin_counter = {\"agent_0\":2, \"agent_1\":1, \"agent_2\":0, \"agent_3\":0, \"agent_4\":0, \"agent_5\":0}\\nenv.change_pin_flag = {\"agent_0\":True, \"agent_1\":True, \"agent_2\":False, \"agent_3\":False, \"agent_4\":False, \"agent_5\":False}\\naction = {\"agent_0\":0, \"agent_1\":1, \"agent_2\":2, \"agent_3\":3, \"agent_4\":0}\\nenv.step(action)'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nets = [[(2,1), (2,3), (3,3)],[(1,4), (3,4), (3,2)], [(1,5), (1,2)], [(0,5), (3,5), (2,2)], [(3,1), (4,3)], [(0,0), (5,0), (5,5)]]\n",
    "macros = [(0,2), (0,4)]\n",
    "length = 6\n",
    "width = 6\n",
    "n_nets = len(nets)\n",
    "edge_capacity = np.full((length,width,4),n_nets)\n",
    "max_step = 1000\n",
    "\n",
    "env = RtGridEnv(length, width, nets, macros, edge_capacity, max_step)\n",
    "obs = env.reset()\n",
    "print(obs)\n",
    "\n",
    "'''env.step_counter = 999\n",
    "env.pin_counter = {\"agent_0\":2, \"agent_1\":1, \"agent_2\":0, \"agent_3\":0, \"agent_4\":0, \"agent_5\":0}\n",
    "env.change_pin_flag = {\"agent_0\":True, \"agent_1\":True, \"agent_2\":False, \"agent_3\":False, \"agent_4\":False, \"agent_5\":False}\n",
    "action = {\"agent_0\":0, \"agent_1\":1, \"agent_2\":2, \"agent_3\":3, \"agent_4\":0}\n",
    "env.step(action)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs  {'agent_0': array([2, 2, 2, 3, 6, 6, 5, 6]), 'agent_1': array([2, 4, 3, 4, 6, 6, 6, 5]), 'agent_2': array([1, 4, 1, 2, 5, 5, 6, 6]), 'agent_3': array([1, 5, 3, 5, 6, 6, 5, 5]), 'agent_4': array([4, 1, 4, 3, 6, 6, 6, 5]), 'agent_5': array([1, 0, 5, 0, 6, 6, 6, 5])}\n",
      "reward {'agent_0': -1, 'agent_1': -1, 'agent_2': -1, 'agent_3': -1, 'agent_4': -1, 'agent_5': -1}\n",
      "done {'agent_0': False, 'agent_1': False, 'agent_2': False, 'agent_3': False, 'agent_4': False, 'agent_5': False, '__all__': False}\n"
     ]
    }
   ],
   "source": [
    "action = {\"agent_0\":0, \"agent_1\":1, \"agent_2\":2, \"agent_3\":1, \"agent_4\":1, \"agent_5\":1}\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(\"obs \", obs)\n",
    "print(\"reward\", reward)\n",
    "print(\"done\", done)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs  {'agent_0': array([2, 3, 2, 3, 6, 6, 5, 6]), 'agent_1': array([3, 4, 3, 4, 6, 6, 6, 5]), 'agent_2': array([1, 3, 1, 2, 5, 6, 6, 6]), 'agent_3': array([2, 5, 3, 5, 6, 6, 6, 5]), 'agent_4': array([4, 2, 4, 3, 6, 6, 5, 6]), 'agent_5': array([2, 0, 5, 0, 6, 6, 6, 5])}\n",
      "reward {'agent_0': 1000, 'agent_1': 1000, 'agent_2': -1, 'agent_3': -1, 'agent_4': -1, 'agent_5': -1}\n",
      "done {'agent_0': False, 'agent_1': False, 'agent_2': False, 'agent_3': False, 'agent_4': False, 'agent_5': False, '__all__': False}\n"
     ]
    }
   ],
   "source": [
    "action = {\"agent_0\":0, \"agent_1\":1, \"agent_2\":2, \"agent_3\":1, \"agent_4\":0, \"agent_5\":1}\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(\"obs \", obs)\n",
    "print(\"reward\", reward)\n",
    "print(\"done\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs  {'agent_0': array([2, 3, 3, 3, 6, 6, 5, 6]), 'agent_1': array([3, 4, 3, 2, 6, 6, 6, 5]), 'agent_2': array([1, 2, 1, 2, 5, 6, 6, 6]), 'agent_3': array([3, 5, 3, 5, 6, 6, 6, 5]), 'agent_4': array([4, 3, 4, 3, 6, 6, 5, 6]), 'agent_5': array([3, 0, 5, 0, 6, 6, 6, 5])}\n",
      "reward {'agent_1': 0, 'agent_0': 0, 'agent_2': 1000, 'agent_3': 1000, 'agent_4': 1000, 'agent_5': -1}\n",
      "done {'agent_0': False, 'agent_1': False, 'agent_2': True, 'agent_3': False, 'agent_4': True, 'agent_5': False, '__all__': False}\n"
     ]
    }
   ],
   "source": [
    "action = {\"agent_0\":0, \"agent_1\":1, \"agent_2\":2, \"agent_3\":1, \"agent_4\":0, \"agent_5\":1}\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(\"obs \", obs)\n",
    "print(\"reward\", reward)\n",
    "print(\"done\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs  {'agent_0': array([3, 3, 3, 3, 6, 6, 6, 5]), 'agent_1': array([3, 3, 3, 2, 5, 6, 6, 5]), 'agent_2': array([1, 2, 1, 2, 5, 6, 6, 6]), 'agent_3': array([3, 5, 2, 2, 6, 6, 6, 5]), 'agent_4': array([4, 3, 4, 3, 6, 6, 5, 6]), 'agent_5': array([4, 0, 5, 0, 6, 6, 6, 5])}\n",
      "reward {'agent_3': 0, 'agent_0': 1000, 'agent_1': -1, 'agent_5': -1}\n",
      "done {'agent_0': True, 'agent_1': False, 'agent_2': True, 'agent_3': False, 'agent_4': True, 'agent_5': False, '__all__': False}\n"
     ]
    }
   ],
   "source": [
    "action = {\"agent_0\":1, \"agent_1\":2,  \"agent_3\":1, \"agent_5\":1}\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(\"obs \", obs)\n",
    "print(\"reward\", reward)\n",
    "print(\"done\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs  {'agent_0': array([3, 3, 3, 3, 6, 6, 6, 5]), 'agent_1': array([3, 2, 3, 2, 5, 6, 6, 6]), 'agent_2': array([1, 2, 1, 2, 5, 6, 6, 6]), 'agent_3': array([2, 5, 2, 2, 6, 4, 6, 5]), 'agent_4': array([4, 3, 4, 3, 6, 6, 5, 6]), 'agent_5': array([5, 0, 5, 0, 6, 6, 6, 5])}\n",
      "reward {'agent_1': 1000, 'agent_3': -1, 'agent_5': 1000}\n",
      "done {'agent_0': True, 'agent_1': True, 'agent_2': True, 'agent_3': False, 'agent_4': True, 'agent_5': False, '__all__': False}\n"
     ]
    }
   ],
   "source": [
    "action = { \"agent_1\":2,  \"agent_3\":3, \"agent_5\":1}\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(\"obs \", obs)\n",
    "print(\"reward\", reward)\n",
    "print(\"done\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs  {'agent_0': array([3, 3, 3, 3, 6, 6, 6, 5]), 'agent_1': array([3, 2, 3, 2, 5, 6, 6, 6]), 'agent_2': array([1, 2, 1, 2, 5, 6, 6, 6]), 'agent_3': array([2, 4, 2, 2, 5, 5, 6, 5]), 'agent_4': array([4, 3, 4, 3, 6, 6, 5, 6]), 'agent_5': array([5, 0, 5, 5, 6, 6, 6, 5])}\n",
      "reward {'agent_5': 0, 'agent_3': -1}\n",
      "done {'agent_0': True, 'agent_1': True, 'agent_2': True, 'agent_3': False, 'agent_4': True, 'agent_5': False, '__all__': False}\n"
     ]
    }
   ],
   "source": [
    "action = {\"agent_3\":2, \"agent_5\":2}\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(\"obs \", obs)\n",
    "print(\"reward\", reward)\n",
    "print(\"done\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs  {'agent_0': array([3, 3, 3, 3, 6, 6, 6, 5]), 'agent_1': array([3, 2, 3, 2, 5, 6, 6, 6]), 'agent_2': array([1, 2, 1, 2, 5, 6, 6, 6]), 'agent_3': array([2, 3, 2, 2, 5, 5, 5, 6]), 'agent_4': array([4, 3, 4, 3, 6, 6, 5, 6]), 'agent_5': array([5, 0, 5, 5, 6, 6, 6, 5])}\n",
      "reward {'agent_3': -1, 'agent_5': -1}\n",
      "done {'agent_0': True, 'agent_1': True, 'agent_2': True, 'agent_3': False, 'agent_4': True, 'agent_5': False, '__all__': False}\n"
     ]
    }
   ],
   "source": [
    "action = {\"agent_3\":2, \"agent_5\":1}\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(\"obs \", obs)\n",
    "print(\"reward\", reward)\n",
    "print(\"done\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs  {'agent_0': array([3, 3, 3, 3, 6, 6, 6, 5]), 'agent_1': array([3, 2, 3, 2, 5, 6, 6, 6]), 'agent_2': array([1, 2, 1, 2, 5, 6, 6, 6]), 'agent_3': array([2, 2, 2, 2, 4, 6, 5, 6]), 'agent_4': array([4, 3, 4, 3, 6, 6, 5, 6]), 'agent_5': array([5, 1, 5, 5, 6, 6, 5, 6])}\n",
      "reward {'agent_3': 1000, 'agent_5': -1}\n",
      "done {'agent_0': True, 'agent_1': True, 'agent_2': True, 'agent_3': True, 'agent_4': True, 'agent_5': False, '__all__': False}\n"
     ]
    }
   ],
   "source": [
    "action = {\"agent_3\":2, \"agent_5\":0}\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(\"obs \", obs)\n",
    "print(\"reward\", reward)\n",
    "print(\"done\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs  {'agent_0': array([3, 3, 3, 3, 6, 6, 6, 5]), 'agent_1': array([3, 2, 3, 2, 5, 6, 6, 6]), 'agent_2': array([1, 2, 1, 2, 5, 6, 6, 6]), 'agent_3': array([2, 2, 2, 2, 4, 6, 5, 6]), 'agent_4': array([4, 3, 4, 3, 6, 6, 5, 6]), 'agent_5': array([5, 0, 5, 5, 5, 6, 6, 5])}\n",
      "reward {'agent_5': 0}\n",
      "done {'agent_0': True, 'agent_1': True, 'agent_2': True, 'agent_3': True, 'agent_4': True, 'agent_5': True, '__all__': True}\n"
     ]
    }
   ],
   "source": [
    "action = {\"agent_5\":0}\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(\"obs \", obs)\n",
    "print(\"reward\", reward)\n",
    "print(\"done\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_0': [[2, 2, 2], [2, 3]], 'agent_1': [[1, 2, 3], [3, 3, 3]], 'agent_2': [[1, 1, 1, 1]], 'agent_3': [[0, 1, 2, 3], [3, 2, 2, 2, 2]], 'agent_4': [[3, 4, 4, 4]], 'agent_5': [[0, 1, 2, 3, 4, 5], [5, 5, 5, 5, 5, 5, 5]]}\n",
      "{'agent_0': [[1, 2, 3], [3, 3]], 'agent_1': [[4, 4, 4], [4, 3, 2]], 'agent_2': [[5, 4, 3, 2]], 'agent_3': [[5, 5, 5, 5], [5, 5, 4, 3, 2]], 'agent_4': [[1, 1, 2, 3]], 'agent_5': [[0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 0]]}\n"
     ]
    }
   ],
   "source": [
    "print(env.path_x)\n",
    "print(env.path_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_0': array([2, 1, 2, 3, 6, 6, 6, 6]), 'agent_1': array([1, 4, 3, 4, 6, 6, 6, 6]), 'agent_2': array([1, 5, 1, 2, 6, 6, 6, 6]), 'agent_3': array([0, 5, 3, 5, 6, 6, 6, 6]), 'agent_4': array([3, 1, 4, 3, 6, 6, 6, 6]), 'agent_5': array([0, 0, 5, 0, 6, 6, 6, 6])}\n",
      "{'agent_0': [[2], []], 'agent_1': [[1], []], 'agent_2': [[1]], 'agent_3': [[0], []], 'agent_4': [[3]], 'agent_5': [[0], []]}\n",
      "{'agent_0': [[1], []], 'agent_1': [[4], []], 'agent_2': [[5]], 'agent_3': [[5], []], 'agent_4': [[1]], 'agent_5': [[0], []]}\n",
      "{'agent_0': False, 'agent_1': False, 'agent_2': False, 'agent_3': False, 'agent_4': False, 'agent_5': False, '__all__': False}\n",
      "{'agent_0': 0, 'agent_1': 0, 'agent_2': 0, 'agent_3': 0, 'agent_4': 0, 'agent_5': 0}\n",
      "{'agent_0': False, 'agent_1': False, 'agent_2': False, 'agent_3': False, 'agent_4': False, 'agent_5': False}\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(obs)\n",
    "print(env.path_x)\n",
    "print(env.path_y)\n",
    "print(env.done_flag)\n",
    "print(env.pin_counter)\n",
    "print(env.change_pin_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change pin flags  {'agent_0': False, 'agent_1': False, 'agent_2': False, 'agent_3': False, 'agent_4': False, 'agent_5': False}\n",
      "done flags  {'agent_0': False, 'agent_1': False, 'agent_2': False, 'agent_3': False, 'agent_4': False, 'agent_5': False, '__all__': False}\n",
      "agent positions  {'agent_0': array([2, 1]), 'agent_1': array([1, 4]), 'agent_2': array([1, 5]), 'agent_3': array([0, 5]), 'agent_4': array([3, 1]), 'agent_5': array([0, 0])}\n",
      "goal positions  {'agent_0': array([2, 3]), 'agent_1': array([3, 4]), 'agent_2': array([1, 2]), 'agent_3': array([3, 5]), 'agent_4': array([4, 3]), 'agent_5': array([5, 0])}\n",
      "path x  {'agent_0': [[2], [], [], [], []], 'agent_1': [[1], []], 'agent_2': [[1], [], []], 'agent_3': [[0], []], 'agent_4': [[3]], 'agent_5': [[0], [], [], []]}\n",
      "path y  {'agent_0': [[1], [], [], [], []], 'agent_1': [[4], []], 'agent_2': [[5], [], []], 'agent_3': [[5], []], 'agent_4': [[1]], 'agent_5': [[0], [], [], []]}\n",
      "resetting...\n",
      "{'agent_0': array([2, 1, 2, 3, 6, 6, 6, 6]), 'agent_1': array([1, 4, 3, 4, 6, 6, 6, 6]), 'agent_2': array([1, 5, 1, 2, 6, 6, 6, 6]), 'agent_3': array([0, 5, 3, 5, 6, 6, 6, 6]), 'agent_4': array([3, 1, 4, 3, 6, 6, 6, 6]), 'agent_5': array([0, 0, 5, 0, 6, 6, 6, 6])}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()\n\u001b[1;32m     22\u001b[0m \u001b[39m#print(action)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m new_obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     24\u001b[0m \u001b[39m#print(new_obs)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[3], line 195\u001b[0m, in \u001b[0;36mRtGridEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action:\u001b[39mdict\u001b[39m):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# extract all the active agents in this time step\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     active_agent \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(action\u001b[39m.\u001b[39;49mkeys())\n\u001b[1;32m    196\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mactive agent \u001b[39m\u001b[39m\"\u001b[39m, active_agent)\n\u001b[1;32m    197\u001b[0m     reward \u001b[39m=\u001b[39m {}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "#nets = [[(2,1), (2,3), (3,3), (4,2)],[(1,4), (3,4), (4,4)]]\n",
    "#macros = [(0,2), (1,2)]\n",
    "nets = [[(2,1), (2,3), (3,3), (4,2), (0,3),(0,1)],[(1,4), (3,4), (3,2)], [(1,5), (1,2), (1,1), (3,1)], [(0,5), (3,5), (2,2)], [(3,1), (4,3)], [(0,0), (5,0), (5,5), (4,5), (4,4)]]\n",
    "macros = [(0,2), (0,4)]\n",
    "length = 6\n",
    "width = 6\n",
    "n_nets = len(nets)\n",
    "edge_capacity = np.full((length,width,4),n_nets)\n",
    "max_step = 1000\n",
    "\n",
    "env = RtGridEnv(length, width, nets, macros, edge_capacity, max_step)\n",
    "num_episodes = 3\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    total_reward = 0\n",
    "    obs = env.reset()\n",
    "    print(\"resetting...\")\n",
    "    print(obs)\n",
    "    done = False\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        #print(action)\n",
    "        new_obs, reward, done, info = env.step(action)\n",
    "        #print(new_obs)\n",
    "        total_reward += reward\n",
    "            \n",
    "        #print(f\"episode: {ep}\")\n",
    "        #print(f\"obs: {new_obs}, reward: {total_reward}, done: {done}\")\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    print(total_reward)\n",
    "    env.render()\n",
    "    env.heatmap()\n",
    "    #print(env.edge_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
