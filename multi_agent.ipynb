{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import heapq\n",
    "import ray\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "from ray import tune, air\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.utils.pre_checks.env import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the custom routing grid environment\n",
    "\n",
    "# action space encoding\n",
    "    # 0 - > move up\n",
    "    # 1 - > move down\n",
    "    # 2 - > move right\n",
    "    # 3 - > move left\n",
    "# state (observation) space encoding\n",
    "    # state[0:1] = current agent position\n",
    "    # state[2:3] = current goal position\n",
    "    # state[4:7] = capacities of the four neighboring edges of the current agent position. Namely, up, right, down, and left\n",
    "\n",
    "\n",
    "class RtGridEnv(MultiAgentEnv):\n",
    "    def __init__(self, length:int, width:int, nets:list, macros:list, edge_capacity:np.ndarray, max_step:int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            length (int): length of the canvas\n",
    "            width (int): width of the canvas\n",
    "            nets (list): a list of nets to be routed\n",
    "            macros (list): a list of macros that has been placed on the canvas by placement\n",
    "        \"\"\"\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "        self.nets = nets\n",
    "        self.n_nets = len(self.nets)\n",
    "        self.macros = macros\n",
    "        self.initial_capacity = edge_capacity.copy()\n",
    "        self.initial_capacity.setflags(write=False)\n",
    "        self.edge_capacity = edge_capacity.copy()\n",
    "        self.max_capacity = np.max(self.edge_capacity) + 1 # plus one to account for the behavior of gym.MultiDiscrete\n",
    "        self.max_step = max_step\n",
    "        self.step_counter = 0 # counts the number of steps elapsed for the current episode\n",
    "        \n",
    "        self.agents_id = []\n",
    "        for i in range(self.n_nets):\n",
    "            self.agents_id.append(\"agent_{}\".format(i))\n",
    "        self.state = {}\n",
    "        self.agent_position = {}\n",
    "        self.goal_position = {}\n",
    "        self.change_pin_flag = self.reset_flags({})\n",
    "        self.done_flag = self.reset_flags({})\n",
    "        #print(\"change pin flags \", self.change_pin_flag)\n",
    "        # the done flag needs an additional \"__all__\" key to indicate all agents are done\n",
    "        self.done_flag[\"__all__\"] = False\n",
    "        #print(\"done flags \", self.done_flag)\n",
    "        self.pin_counter = {}\n",
    "        self.reset_pin_counters()\n",
    "        self.path_x = self.generate_path(self.nets)\n",
    "        self.path_y = self.generate_path(self.nets)\n",
    "        #print(self.path_x)\n",
    "        self.decomposed_nets = {}\n",
    "        for i in range(self.n_nets): \n",
    "            self.decomposed_nets[self.agents_id[i]] = self.prim_mst(self.nets[i])\n",
    "        \n",
    "        # initialize the agent to route the first 2-pin net decomposed from the first multi-pin net\n",
    "        for agent_id in self.agents_id:\n",
    "            self.update_positions(agent_id)\n",
    "            self.update_path(agent_id)\n",
    "        #print(self.agent_position)\n",
    "        #print(self.goal_position)\n",
    "        # initialize the path lists\n",
    "        #for agent_id in self.agents_id:\n",
    "            #self.update_path(agent_id)\n",
    "        #print(self.path_x)\n",
    "        #print(self.path_y)\n",
    "        # define the action and the observation space\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = MultiDiscrete(\n",
    "            [\n",
    "                self.length, \n",
    "                self.width, \n",
    "                self.length, \n",
    "                self.width, \n",
    "                self.max_capacity, \n",
    "                self.max_capacity, \n",
    "                self.max_capacity, \n",
    "                self.max_capacity\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def update_positions(self, agent_id):\n",
    "        \"\"\"\n",
    "        Update the agent position with the starting pin of the next 2-pin net.\n",
    "        Update the goal position with the new goal.\n",
    "        \"\"\"\n",
    "        self.agent_position[agent_id] = np.array(self.decomposed_nets[agent_id]['u'][self.pin_counter[agent_id]])\n",
    "        self.goal_position[agent_id] = np.array(self.decomposed_nets[agent_id]['v'][self.pin_counter[agent_id]])\n",
    "\n",
    "    def update_path(self, agent_id):\n",
    "        \"\"\"Update the path agent has traveled.\"\"\"\n",
    "        self.path_x[agent_id][self.pin_counter[agent_id]].append(self.agent_position[agent_id][0])\n",
    "        self.path_y[agent_id][self.pin_counter[agent_id]].append(self.agent_position[agent_id][1])\n",
    "\n",
    "    def reset_pin_counters(self):\n",
    "        \"\"\"Set the pin counter of each agent to 0.\"\"\"\n",
    "        for i in range(self.n_nets):\n",
    "            self.pin_counter[self.agents_id[i]] = 0\n",
    "    \n",
    "    def reset_flags(self, flags:dict):\n",
    "        for i in range(self.n_nets):\n",
    "            flags[self.agents_id[i]] = False\n",
    "\n",
    "        return flags\n",
    "\n",
    "    def generate_path(self, nets:list):\n",
    "        \"\"\"Generate the list data structure to hold the path traveled by the agent.\"\"\"\n",
    "        path = {}\n",
    "        for i in range(len(nets)):\n",
    "            path[self.agents_id[i]] = []\n",
    "            for j in range(len(nets[i])-1):\n",
    "                path[self.agents_id[i]].append([])\n",
    "\n",
    "        return path\n",
    "\n",
    "    def prim_mst(self, pins):\n",
    "        \"\"\"\n",
    "        Compute the Minimum Spanning Tree (MST) using Prim's algorithm.\n",
    "\n",
    "        Args:\n",
    "            pins (list): List of (x, y) coordinates representing the pin locations.\n",
    "\n",
    "        Returns:\n",
    "            dict: a dictionary containing the vertices of all the edges in the MST\n",
    "\n",
    "        Note:\n",
    "            - The pins list should contain at least two points.\n",
    "        \"\"\"\n",
    "\n",
    "        def euclidean_distance(p1, p2):\n",
    "            \"\"\"\n",
    "            Compute the Euclidean distance between two points.\n",
    "\n",
    "            Args:\n",
    "                p1 (tuple): First point (x, y) coordinates.\n",
    "                p2 (tuple): Second point (x, y) coordinates.\n",
    "\n",
    "            Returns:\n",
    "                float: Euclidean distance between the two points.\n",
    "            \"\"\"\n",
    "            x1, y1 = p1\n",
    "            x2, y2 = p2\n",
    "            return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "        \n",
    "        distances = {}\n",
    "        for i in range(len(pins)):\n",
    "            for j in range(i+1, len(pins)):\n",
    "                p1 = pins[i]\n",
    "                p2 = pins[j]\n",
    "                distances[(i, j)] = euclidean_distance(p1, p2)\n",
    "                distances[(j, i)] = distances[(i, j)]  # Add symmetric distance\n",
    "        \n",
    "        # Initialize\n",
    "        num_pins = len(pins)\n",
    "        visited = [False] * num_pins\n",
    "        mst_u = []\n",
    "        mst_v = []\n",
    "        start_vertex = 0\n",
    "        visited[start_vertex] = True\n",
    "        \n",
    "        # Create a priority queue\n",
    "        pq = []\n",
    "        \n",
    "        # Mark the initial vertex as visited\n",
    "        for i in range(num_pins):\n",
    "            if i != start_vertex:\n",
    "                heapq.heappush(pq, (distances[(start_vertex, i)], start_vertex, i))\n",
    "        \n",
    "        # Update the priority queue and perform Prim's algorithm\n",
    "        while pq:\n",
    "            if (len(mst_u) == len(pins) -1): # for n pins, the MST should at most have n-1 edges\n",
    "                break\n",
    "\n",
    "            weight, u, v = heapq.heappop(pq)\n",
    "            \n",
    "            if visited[v]:\n",
    "                #print(f\"Skipping edge: {weight} - {u} - {v}\")\n",
    "                continue\n",
    "            \n",
    "            # Prim's algorithm iteration\n",
    "            visited[v] = True\n",
    "            mst_u.append(pins[u])\n",
    "            mst_v.append(pins[v])\n",
    "            \n",
    "            for i in range(num_pins):\n",
    "                if not visited[i]:\n",
    "                    heapq.heappush(pq, (distances[(v, i)], v, i))\n",
    "        \n",
    "        mst = {'u':mst_u,'v':mst_v}\n",
    "\n",
    "        return mst\n",
    "    \n",
    "    def step(self, action:dict):\n",
    "        # extract all the active agents in this time step\n",
    "        active_agent = list(action.keys())\n",
    "        #print(\"active agent \", active_agent)\n",
    "        reward = {}\n",
    "\n",
    "        # if we have reached our maximum time step, set the all done flag\n",
    "        self.step_counter += 1\n",
    "        if self.step_counter >= self.max_step:\n",
    "            for agent_id in active_agent:\n",
    "                reward[agent_id] = -1\n",
    "            self.done_flag[\"__all__\"] = True\n",
    "            #print(\"reward \", reward)\n",
    "            #print(\"done flags \", self.done_flag)\n",
    "            return self.state, reward, self.done_flag, {}\n",
    "        \n",
    "        # update pins for those agents in need\n",
    "        pin_flag_agents = [key for key, value in self.change_pin_flag.items() if value]\n",
    "        for agent_id in pin_flag_agents:\n",
    "            self.update_positions(agent_id)\n",
    "            self.update_path(agent_id)\n",
    "            reward[agent_id] = 0\n",
    "            self.change_pin_flag[agent_id] = False\n",
    "            self.state[agent_id] = np.concatenate([\n",
    "                self.agent_position[agent_id], \n",
    "                self.goal_position[agent_id], \n",
    "                self.edge_capacity[self.agent_position[agent_id][0]][self.agent_position[agent_id][1]]\n",
    "                ])\n",
    "        #print(\"state \", self.state)\n",
    "        #print(\"change pin flag \", self.change_pin_flag)\n",
    "        #print(\"reward \", reward)\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        #print(\"RESETTING...\")\n",
    "        self.reset_pin_counters()\n",
    "        #print(\"pin counters \", self.pin_counter)\n",
    "        self.step_counter = 0\n",
    "        self.change_pin_flag = self.reset_flags(self.change_pin_flag)\n",
    "        self.done_flag = self.reset_flags(self.done_flag)\n",
    "        self.done_flag[\"__all__\"] = False\n",
    "        #print(\"change pin flags \", self.change_pin_flag)\n",
    "        #print(\"done flags \", self.done_flag)\n",
    "        self.edge_capacity = self.initial_capacity.copy()\n",
    "        self.path_x = self.generate_path(self.nets)\n",
    "        self.path_y = self.generate_path(self.nets)\n",
    "        #print(\"path x \", self.path_x)\n",
    "        #print(\"path y \", self.path_y)\n",
    "        for agent_id in self.agents_id:\n",
    "            self.update_positions(agent_id)\n",
    "            self.update_path(agent_id)\n",
    "        #for agent_id in self.agents_id:\n",
    "            #self.update_path(agent_id)\n",
    "        #print(\"agent positions \", self.agent_position)\n",
    "        #print(\"goal positions \", self.goal_position)\n",
    "        #print(\"path x \", self.path_x)\n",
    "        #print(\"path y \", self.path_y)\n",
    "        for agent_id in self.agents_id:\n",
    "            individual_state = np.concatenate([\n",
    "                self.agent_position[agent_id], \n",
    "                self.goal_position[agent_id], \n",
    "                self.edge_capacity[self.agent_position[agent_id][0]][self.agent_position[agent_id][1]]\n",
    "                ])\n",
    "            self.state[agent_id] = individual_state\n",
    "        \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_0': array([2, 1, 2, 3, 6, 6, 6, 6]), 'agent_1': array([1, 4, 3, 4, 6, 6, 6, 6]), 'agent_2': array([1, 5, 1, 2, 6, 6, 6, 6]), 'agent_3': array([0, 5, 3, 5, 6, 6, 6, 6]), 'agent_4': array([3, 1, 4, 3, 6, 6, 6, 6]), 'agent_5': array([0, 0, 5, 0, 6, 6, 6, 6])}\n",
      "active agent  ['agent_0', 'agent_1', 'agent_2', 'agent_3', 'agent_4']\n",
      "reward  {'agent_0': -1, 'agent_1': -1, 'agent_2': -1, 'agent_3': -1, 'agent_4': -1}\n",
      "done flags  {'agent_0': False, 'agent_1': False, 'agent_2': False, 'agent_3': False, 'agent_4': False, 'agent_5': False, '__all__': True}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'agent_0': array([2, 1, 2, 3, 6, 6, 6, 6]),\n",
       "  'agent_1': array([1, 4, 3, 4, 6, 6, 6, 6]),\n",
       "  'agent_2': array([1, 5, 1, 2, 6, 6, 6, 6]),\n",
       "  'agent_3': array([0, 5, 3, 5, 6, 6, 6, 6]),\n",
       "  'agent_4': array([3, 1, 4, 3, 6, 6, 6, 6]),\n",
       "  'agent_5': array([0, 0, 5, 0, 6, 6, 6, 6])},\n",
       " {'agent_0': -1, 'agent_1': -1, 'agent_2': -1, 'agent_3': -1, 'agent_4': -1},\n",
       " {'agent_0': False,\n",
       "  'agent_1': False,\n",
       "  'agent_2': False,\n",
       "  'agent_3': False,\n",
       "  'agent_4': False,\n",
       "  'agent_5': False,\n",
       "  '__all__': True},\n",
       " {})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nets = [[(2,1), (2,3), (3,3), (4,2), (0,3),(0,1)],[(1,4), (3,4), (3,2)], [(1,5), (1,2), (1,1), (3,1)], [(0,5), (3,5), (2,2)], [(3,1), (4,3)], [(0,0), (5,0), (5,5), (4,5), (4,4)]]\n",
    "macros = [(0,2), (0,4)]\n",
    "length = 6\n",
    "width = 6\n",
    "n_nets = len(nets)\n",
    "edge_capacity = np.full((length,width,4),n_nets)\n",
    "max_step = 1000\n",
    "\n",
    "env = RtGridEnv(length, width, nets, macros, edge_capacity, max_step)\n",
    "obs = env.reset()\n",
    "print(obs)\n",
    "env.step_counter = 999\n",
    "env.pin_counter = {\"agent_0\":2, \"agent_1\":1, \"agent_2\":0, \"agent_3\":0, \"agent_4\":0, \"agent_5\":0}\n",
    "env.change_pin_flag = {\"agent_0\":True, \"agent_1\":True, \"agent_2\":False, \"agent_3\":False, \"agent_4\":False, \"agent_5\":False}\n",
    "action = {\"agent_0\":0, \"agent_1\":1, \"agent_2\":2, \"agent_3\":3, \"agent_4\":0}\n",
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change pin flags  {'agent_0': False, 'agent_1': False, 'agent_2': False, 'agent_3': False, 'agent_4': False, 'agent_5': False}\n",
      "done flags  {'agent_0': False, 'agent_1': False, 'agent_2': False, 'agent_3': False, 'agent_4': False, 'agent_5': False, '__all__': False}\n",
      "agent positions  {'agent_0': array([2, 1]), 'agent_1': array([1, 4]), 'agent_2': array([1, 5]), 'agent_3': array([0, 5]), 'agent_4': array([3, 1]), 'agent_5': array([0, 0])}\n",
      "goal positions  {'agent_0': array([2, 3]), 'agent_1': array([3, 4]), 'agent_2': array([1, 2]), 'agent_3': array([3, 5]), 'agent_4': array([4, 3]), 'agent_5': array([5, 0])}\n",
      "path x  {'agent_0': [[2], [], [], [], []], 'agent_1': [[1], []], 'agent_2': [[1], [], []], 'agent_3': [[0], []], 'agent_4': [[3]], 'agent_5': [[0], [], [], []]}\n",
      "path y  {'agent_0': [[1], [], [], [], []], 'agent_1': [[4], []], 'agent_2': [[5], [], []], 'agent_3': [[5], []], 'agent_4': [[1]], 'agent_5': [[0], [], [], []]}\n",
      "resetting...\n",
      "{'agent_0': array([2, 1, 2, 3, 6, 6, 6, 6]), 'agent_1': array([1, 4, 3, 4, 6, 6, 6, 6]), 'agent_2': array([1, 5, 1, 2, 6, 6, 6, 6]), 'agent_3': array([0, 5, 3, 5, 6, 6, 6, 6]), 'agent_4': array([3, 1, 4, 3, 6, 6, 6, 6]), 'agent_5': array([0, 0, 5, 0, 6, 6, 6, 6])}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()\n\u001b[1;32m     22\u001b[0m \u001b[39m#print(action)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m new_obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     24\u001b[0m \u001b[39m#print(new_obs)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[3], line 195\u001b[0m, in \u001b[0;36mRtGridEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action:\u001b[39mdict\u001b[39m):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# extract all the active agents in this time step\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     active_agent \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(action\u001b[39m.\u001b[39;49mkeys())\n\u001b[1;32m    196\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mactive agent \u001b[39m\u001b[39m\"\u001b[39m, active_agent)\n\u001b[1;32m    197\u001b[0m     reward \u001b[39m=\u001b[39m {}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "#nets = [[(2,1), (2,3), (3,3), (4,2)],[(1,4), (3,4), (4,4)]]\n",
    "#macros = [(0,2), (1,2)]\n",
    "nets = [[(2,1), (2,3), (3,3), (4,2), (0,3),(0,1)],[(1,4), (3,4), (3,2)], [(1,5), (1,2), (1,1), (3,1)], [(0,5), (3,5), (2,2)], [(3,1), (4,3)], [(0,0), (5,0), (5,5), (4,5), (4,4)]]\n",
    "macros = [(0,2), (0,4)]\n",
    "length = 6\n",
    "width = 6\n",
    "n_nets = len(nets)\n",
    "edge_capacity = np.full((length,width,4),n_nets)\n",
    "max_step = 1000\n",
    "\n",
    "env = RtGridEnv(length, width, nets, macros, edge_capacity, max_step)\n",
    "num_episodes = 3\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    total_reward = 0\n",
    "    obs = env.reset()\n",
    "    print(\"resetting...\")\n",
    "    print(obs)\n",
    "    done = False\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        #print(action)\n",
    "        new_obs, reward, done, info = env.step(action)\n",
    "        #print(new_obs)\n",
    "        total_reward += reward\n",
    "            \n",
    "        #print(f\"episode: {ep}\")\n",
    "        #print(f\"obs: {new_obs}, reward: {total_reward}, done: {done}\")\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    print(total_reward)\n",
    "    env.render()\n",
    "    env.heatmap()\n",
    "    #print(env.edge_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
