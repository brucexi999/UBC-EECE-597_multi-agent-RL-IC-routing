@article{Schulman2017,
   abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
   author = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
   month = {7},
   title = {Proximal Policy Optimization Algorithms},
   url = {http://arxiv.org/abs/1707.06347},
   year = {2017},
}
@misc{Wang2018,
   abstract = {We address the problem of learning structured policies for continuous control. In traditional reinforcement learning, policies of agents are learned by multi-layer perceptrons (MLPs) which take the concatenation of all observations from the environment as input for predicting actions. In this work, we propose NerveNet to explicitly model the structure of an agent, which naturally takes the form of a graph. Specifically, serving as the agent's policy network, NerveNet first propagates information over the structure of the agent and then predict actions for different parts of the agent. In the experiments, we first show that our NerveNet is comparable to state-of-the-art methods on standard MuJoCo environments. We further propose our customized reinforcement learning environments for benchmarking two types of structure transfer learning tasks, i.e., size and disability transfer, as well as multi-task learning. We demonstrate that policies learned by NerveNet are significantly more transferable and generalizable than policies learned by other models and are able to transfer even in a zero-shot setting.},
   author = {Tingwu Wang and Renjie Liao and Jimmy Ba and Sanja Fidler},
   title = {NERVENET: LEARNING STRUCTURED POLICY WITH GRAPH NEURAL NETWORKS},
   url = {http://www.cs.toronto.edu/},
   year = {2018},
}
@article{Almasan2022,
   abstract = {Deep Reinforcement Learning (DRL) has shown a dramatic improvement in decision-making and automated control problems. Consequently, DRL represents a promising technique to efficiently solve many relevant optimization problems (e.g., routing) in self-driving networks. However, existing DRL-based solutions applied to networking fail to generalize, which means that they are not able to operate properly when applied to network topologies not observed during training. This lack of generalization capability significantly hinders the deployment of DRL technologies in production networks. This is because state-of-the-art DRL-based networking solutions use standard neural networks (e.g., fully connected, convolutional), which are not suited to learn from information structured as graphs. In this paper, we integrate Graph Neural Networks (GNN) into DRL agents and we design a problem specific action space to enable generalization. GNNs are Deep Learning models inherently designed to generalize over graphs of different sizes and structures. This allows the proposed GNN-based DRL agent to learn and generalize over arbitrary network topologies. We test our DRL+GNN agent in a routing optimization use case in optical networks and evaluate it on 180 and 232 unseen synthetic and real-world network topologies respectively. The results show that the DRL+GNN agent is able to outperform state-of-the-art solutions in topologies never seen during training.},
   author = {Paul Almasan and José Suárez-Varela and Krzysztof Rusek and Pere Barlet-Ros and Albert Cabellos-Aparicio},
   doi = {10.1016/j.comcom.2022.09.029},
   issn = {1873703X},
   journal = {Computer Communications},
   keywords = {Deep reinforcement learning,Graph neural networks,Optimization,Routing},
   month = {12},
   pages = {184-194},
   publisher = {Elsevier B.V.},
   title = {Deep reinforcement learning meets graph neural networks: Exploring a routing optimization use case},
   volume = {196},
   year = {2022},
}
@book{SB,
   abstract = {Second edition. "Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms"--Provided by publisher. Introduction. Reinforcement Learning -- Examples -- Elements of Reinforcement Learning -- Limitations and Scope -- An Extended Example: Tic-Tac-Toe -- Summary -- Early History of Reinforcement Learning -- Tabular Solution Methods. Multi-armed Bandits. A k-armed Bandit Problem -- Action-value Methods -- The 10-armed Testbed -- Incremental Implementation -- Tracking a Nonstationary Problem -- Optimistic Initial Values -- Upper-Confidence-Bound Action Selection -- Gradient Bandit Algorithms -- Associative Search (Contextual Bandits) -- Summary -- Finite Markov Decision Processes -- The Agent-Environment Interface -- Goals and Rewards -- Returns and Episodes -- Unified Notation for Episodic and Continuing Tasks -- Policies and Value Functions -- Optimal Policies and Optimal Value Functions -- Optimality and Approximation -- Summary -- Dynamic Programming. Policy Evaluation (Prediction) -- Policy Improvement -- Policy Iteration -- Value Iteration -- Asynchronous Dynamic Programming -- Generalized Policy Iteration -- Efficiency of Dynamic Programming -- Summary -- Monte Carlo Methods. Monte Carlo Prediction -- Monte Carlo Estimation of Action Values -- Monte Carlo Control -- Monte Carlo Control without Exploring Starts -- Off-policy Prediction via Importance Sampling -- Incremental Implementation -- Off-policy Monte Carlo Control -- *Discounting-aware Importance Sampling -- *Per-decision Importance Sampling -- Summary -- Temporal-Difference Learning. TD Prediction -- Advantages of TD Prediction Methods -- Optimality of TD(0) -- Sarsa: On-policy TD Control -- Q-learning: Off-policy TD Control -- Expected Sarsa -- Maximization Bias and Double Learning Games, Afterstates, and Other Special Cases -- Summary -- n-step Bootstrapping. n-step TD Prediction -- n-step Sarsa -- n-step Off-policy Learning -- *Per-decision Methods with Control Variates -- Off-policy Learning Without Importance Sampling: The n-step Tree Backup Algorithm -- *A Unifying Algorithm: n-step Q(u) -- Summary -- Planning and Learning with Tabular Methods. Models and Planning -- Dyna: Integrated Planning, Acting, and Learning -- When the Model Is Wrong -- Prioritized Sweeping -- Expected vs. Sample Updates -- Trajectory Sampling -- Real-time Dynamic Programming -- Planning at Decision Time -- Heuristic Search -- Rollout Algorithms -- Monte Carlo Tree Search -- Summary of the Chapter -- Summary of Part I: Dimensions -- Approximate Solution Methods. On-policy Prediction with Approximation. Value-function Approximation -- The Prediction Objective (VE) Stochastic-gradient and Semi-gradient Methods -- Linear Methods -- Feature Construction for Linear Methods -- Polynomials -- Fourier Basis -- Coarse Coding -- Tile Coding -- Radial Basis Functions -- Selecting Step-Size Parameters Manually -- Nonlinear Function Approximation: Artificial Neural Networks -- Least-Squares TD -- Memory-based Function Approximation -- Kernel-based Function Approximation -- Looking Deeper at On-policy Learning: Interest and Emphasis -- Summary -- On-policy Control with Approximation. Episodic Semi-gradient Control -- Semi-gradient n-step Sarsa -- Average Reward: A New Problem Setting for Continuing Tasks -- Deprecating the Discounted Setting -- Differential Semi-gradient n-step Sarsa -- Summary -- *Off-policy Methods with Approximation. Semi-gradient Methods -- Examples of Off-policy Divergence The Deadly Triad -- Linear Value-function Geometry -- Gradient Descent in the Bellman Error -- The Bellman Error is Not Learnable -- Gradient-TD Methods -- Emphatic-TD Methods -- Reducing Variance -- Summary -- Eligibility Traces. The A-return -- TD(A) -- n-step Truncated A-return Methods -- Redoing Updates: Online A-return Algorithm -- True Online TD(A) -- *Dutch Traces in Monte Carlo Learning -- Sarsa(A) -- Variable A and ry -- Off-policy Traces with Control Variates -- Watkins's Q(A) to Tree-Backup(A) -- Stable Off-policy Methods with Traces -- Implementation Issues -- Conclusions -- Policy Gradient Methods. Policy Approximation and its Advantages -- The Policy Gradient Theorem -- REINFORCE: Monte Carlo Policy Gradient -- REINFORCE with Baseline -- Actor-Critic Methods Policy Gradient for Continuing Problems -- Policy Parameterization for Continuous Actions -- Summary -- Looking Deeper. Psychology. Prediction and Control -- Classical Conditioning -- Blocking and Higher-order Conditioning -- The Rescorla-Wagner Model -- The TD Model -- TD Model Simulations -- Instrumental Conditioning -- Delayed Reinforcement -- Cognitive Maps -- Habitual and Goal-directed Behavior -- Summary -- Neuroscience -- Neuroscience Basics -- Reward Signals, Reinforcement Signals, Values, and Prediction Errors -- The Reward Prediction Error Hypothesis -- Dopamine -- Experimental Support for the Reward Prediction Error Hypothesis -- TD Error/Dopamine Correspondence -- Neural Actor-Critic -- Actor and Critic Learning Rules -- Hedonistic Neurons -- Collective Reinforcement Learning -- Model-based Methods in the Brain Addiction -- Summary -- Applications and Case Studies. TD-Gammon -- Samuel's Checkers Player -- Watson's Daily-Double Wagering -- Optimizing Memory Control -- Human-level Video Game Play -- Mastering the Game of Go -- AlphaGo -- AlphaGo Zero -- Personalized Web Services -- Thermal Soaring -- Frontiers. General Value Functions and Auxiliary Tasks -- Temporal Abstraction via Options -- Observations and State -- Designing Reward Signals -- Remaining Issues -- Experimental Support for the Reward Prediction Error Hypothesis.},
   author = {Richard S. Sutton and Andrew G. Barto},
   city = {Cambridge, Massachusetts},
   edition = {Second edition},
   isbn = {9780262039246},
   pages = {526},
   publisher = {The MIT Press},
   title = {Reinforcement learning : an introduction},
}
@article{Kramer1984,
   author = {M.R. Kramer and J.van Leeuwen},
   journal = {Advances in Computing Research},
   pages = {129-146},
   title = {The Complexity of Wirerouting and Finding Minimum Area Layouts for Arbitrary VLSI Circuits},
   volume = {2},
   year = {1984},
}
@article{Hu2001,
   abstract = {This paper presents a comprehensive survey on global routing research over about the last two decades, with an emphasis on the problems of simultaneously routing multiple nets in VLSI circuits under various design styles. The survey begins with a coverage of traditional approaches such as sequential routing and rip-up-and-reroute, and then discusses multicommodity flow-based methods, which have attracted a good deal of attention recently. The family of hierarchical routing techniques and several of its variants are then overviewed, in addition to other techniques such as move-based heuristics and iterative deletion. While many traditional techniques focus on the conventional objective of managing congestion, newer objectives have come into play with the advances in VLSI technology. Specifically, the focus of global routing has shifted so that it is important to augment the congestion objective with metrics for timing and crosstalk. In the later part of this paper, we summarize the recent progress in these directions. Finally, the survey concludes with a summary of possible future research directions.[rule] © 2001 Elsevier Science B.V. All rights reserved.},
   author = {Jiang Hu and Sachin S. Sapatnekar},
   doi = {10.1016/S0167-9260(01)00020-7},
   issn = {0167-9260},
   issue = {1},
   journal = {Integration},
   month = {11},
   pages = {1-49},
   publisher = {Elsevier},
   title = {A survey on multi-net global routing for integrated circuits},
   volume = {31},
   year = {2001},
}
@misc{Mnih2013,
   abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
   author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
   title = {Playing Atari with Deep Reinforcement Learning},
   year = {2013},
}
@misc{Cho2006,
   abstract = {In this paper, we propose a new global router, BoxRouter, powered by the concept of box expansion and progressive integer linear programming (ILP). BoxRouter first uses a simple PreRouting strategy which can predict and capture the most congested regions with high fidelity compared to the final routing. Based on progressive box expansion initiated from the most congested region, BoxRouting is performed with progressive ILP and adaptive maze routing. It is followed by an effective PostRouting step which reroutes without rip-up to obtain smooth tradeoff between wirelength and routability. Our experimental results show that BoxRouter significantly outperforms the state-of-the-art published global routers, e.g., 79% better routability than [1] (with similar wirelength and 2x speedup), 4.2% less wirelength and 16x speedup than [2] (with similar routability). Given the fundamental importance of routing, such dramatic improvement shall sparkle renewed interests in routing which plays a key role in nanometer design and manufacturing closure.},
   author = {Minsik Cho and David Z Pan},
   keywords = {B72 [Hardware,Congestion,Design,Global Routing,Integrated Circuit]: Design Aids General Terms Algorithms,Performance Keywords VLSI},
   title = {BoxRouter: A New Global Router Based on Box Expansion and Progressive ILP},
   year = {2006},
}
@misc{Fragiadakis2015,
   author = {Michalis Fragiadakis and Symeon Christodoulou},
   title = {Generating a seismic reliability assessment for an urban water distribution network},
   url = {https://www.researchgate.net/publication/288975406},
   year = {2015},
}
@inproceedings{Fortz2000,
   abstract = {Open Shortest Path First (OSPF) is the most commonly used intra-domain Internet routing protocol. An AT&T WorldNet backbone with demands projected from previous measurements was used to optimize the weight setting based on the projected demands. Optimizing the weight settings for a given set of demands is NP-hard, hence, local search heuristic was used. Results show that the AT&T WorldNet backbone weight settings performed within a few percent from that of the optimal general routing where the flow for each demand is optimally distributed over all paths between source and destination.},
   author = {Bernard Fortz and Mikkel Thorup},
   doi = {10.1109/infcom.2000.832225},
   issn = {0743166X},
   journal = {Proceedings - IEEE INFOCOM},
   pages = {519-528},
   publisher = {IEEE},
   title = {Internet traffic engineering by optimizing OSPF weights},
   volume = {2},
   year = {2000},
}
@article{Xu2023,
   abstract = {Global routing is a crucial step in the design of Very Large-Scale Integration (VLSI) circuits. However, most of the existing methods are heuristic algorithms, which cannot conjointly optimize the subproblems of global routing, resulting in congestion and overflow. In response to this challenge, an enhanced Deep Reinforcement Learning- (DRL-) based global router has been proposed, which comprises the following effective strategies. First, to avoid the overestimation problem generated by Q-learning, the proposed global router adopts the Double Deep Q-Network (DDQN) model. The DDQN-based global router has better performance in wire length optimization and convergence. Second, to avoid the agent from learning redundant information, an action elimination method is added to the action selection part, which significantly enhances the convergence performance of the training process. Third, to avoid the unfair allocation problem of routing resources in serial training, concurrent training is proposed to enhance the routability. Fourth, to reduce wire length and disperse routing resources, a new reward function is proposed to guide the agent to learn better routing solutions regarding wire length and congestion standard deviation. Experimental results demonstrate that the proposed algorithm outperforms others in several important performance metrics, including wire length, convergence performance, routability, and congestion standard deviation. In conclusion, the proposed enhanced DRL-based global router is a promising approach for solving the global routing problem in VLSI design, which can achieve superior performance compared to the heuristic method and DRL-based global router.},
   author = {Saijuan Xu and Liliang Yang and Genggeng Liu},
   doi = {10.1155/2023/6593938},
   issn = {15308677},
   journal = {Wireless Communications and Mobile Computing},
   publisher = {Hindawi Limited},
   title = {An Enhanced Deep Reinforcement Learning-Based Global Router for VLSI Design},
   volume = {2023},
   year = {2023},
}
@article{Liao2020,
   abstract = {Global routing has been a historically challenging problem in the electronic circuit design, where the challenge is to connect a large and arbitrary number of circuit components with wires without violating the design rules for the printed circuit boards or integrated circuits. Similar routing problems also exist in the design of complex hydraulic systems, pipe systems, and logistic networks. Existing solutions typically consist of greedy algorithms and hard-coded heuristics. As such, existing approaches suffer from a lack of model flexibility and usually fail to solve sub-problems conjointly. As an alternative approach, this work presents a deep reinforcement learning method for solving the global routing problem in a simulated environment. At the heart of the proposed method is deep reinforcement learning that enables an agent to produce a policy for routing based on the variety of problems, and it is presented with leveraging the conjoint optimization mechanism of deep reinforcement learning. Conjoint optimization mechanism is explained and demonstrated in detail; the best network structure and the parameters of the learned model are explored. Based on the fine-tuned model, routing solutions and rewards are presented and analyzed. The results indicate that the approach can outperform the benchmark method of a sequential A*method, suggesting a promising potential for deep reinforcement learning for global routing and other routing or path planning problems in general. Another major contribution of this work is the development of a global routing problem sets generator with the ability to generate parameterized global routing problem sets with different size and constraints, enabling evaluation of different routing algorithms and the generation of training datasets for future data-driven routing approaches.},
   author = {Haiguang Liao and Wentai Zhang and Xuliang Dong and Barnabas Poczos and Kenji Shimada and Levent Burak Kara},
   doi = {10.1115/1.4045044},
   issn = {10500472},
   issue = {6},
   journal = {Journal of Mechanical Design, Transactions of the ASME},
   keywords = {agent-based design,artificial intelligence,design automation,machine learning},
   month = {6},
   publisher = {American Society of Mechanical Engineers (ASME)},
   title = {A Deep Reinforcement Learning Approach for Global Routing},
   volume = {142},
   year = {2020},
}
@article{Mirhoseini2021,
   abstract = {Chip floorplanning is the engineering task of designing the physical layout of a computer chip. Despite five decades of research1, chip floorplanning has defied automation, requiring months of intense effort by physical design engineers to produce manufacturable layouts. Here we present a deep reinforcement learning approach to chip floorplanning. In under six hours, our method automatically generates chip floorplans that are superior or comparable to those produced by humans in all key metrics, including power consumption, performance and chip area. To achieve this, we pose chip floorplanning as a reinforcement learning problem, and develop an edge-based graph convolutional neural network architecture capable of learning rich and transferable representations of the chip. As a result, our method utilizes past experience to become better and faster at solving new instances of the problem, allowing chip design to be performed by artificial agents with more experience than any human designer. Our method was used to design the next generation of Google’s artificial intelligence (AI) accelerators, and has the potential to save thousands of hours of human effort for each new generation. Finally, we believe that more powerful AI-designed hardware will fuel advances in AI, creating a symbiotic relationship between the two fields.},
   author = {Azalia Mirhoseini and Anna Goldie and Mustafa Yazgan and Joe Wenjie Jiang and Ebrahim Songhori and Shen Wang and Young Joon Lee and Eric Johnson and Omkar Pathak and Azade Nazi and Jiwoo Pak and Andy Tong and Kavya Srinivasa and William Hang and Emre Tuncer and Quoc V. Le and James Laudon and Richard Ho and Roger Carpenter and Jeff Dean},
   doi = {10.1038/s41586-021-03544-w},
   issn = {14764687},
   issue = {7862},
   journal = {Nature},
   month = {6},
   pages = {207-212},
   pmid = {34108699},
   publisher = {Nature Research},
   title = {A graph placement methodology for fast chip design},
   volume = {594},
   year = {2021},
}
