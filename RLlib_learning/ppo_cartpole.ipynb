{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0956ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray: 2.2.0\n",
      "gym: 0.21.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import gym\n",
    "import ray\n",
    "from ray import tune, rllib, air\n",
    "#from ray.tune.logger import pretty_print\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.utils.pre_checks.env import check_env\n",
    "from IPython.display import display\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "print(f\"gym: {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b455a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the cartpole environment\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33fed9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space:  Discrete(2)\n",
      "observation space:  Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "# inspect the environment\n",
    "print(\"action space: \", env.action_space)\n",
    "print(\"observation space: \", env.observation_space)\n",
    "num_episodes = 5\n",
    "total_reward = 0\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        new_obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        #print(f\"episode: {ep}\")\n",
    "        #print(f\"obs: {new_obs}, reward: {total_reward}, done: {done}\")\n",
    "        env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e14dfecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking environment ...\n",
      "All checks passed. No errors found.\n"
     ]
    }
   ],
   "source": [
    "print(\"checking environment ...\")\n",
    "try:\n",
    "    check_env(env)\n",
    "    print(\"All checks passed. No errors found.\")\n",
    "except:\n",
    "    print(\"failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e1891ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************\n",
      "Baseline Mean Reward=22.25+/-11.66\n",
      "Baseline won 66745.0 times over 3000 episodes (66745 timesteps)\n",
      "Approx 22.25 wins per episode\n",
      "**************\n"
     ]
    }
   ],
   "source": [
    "# calculate the environment baseline\n",
    "num_episodes = 3000\n",
    "num_timesteps = 0\n",
    "episode_rewards = []\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0.0\n",
    "\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        new_obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        num_timesteps += 1\n",
    "\n",
    "        if done:\n",
    "            episode_rewards.append(episode_reward)\n",
    "            break\n",
    "\n",
    "# calculate mean_reward\n",
    "env_mean_random_reward = np.mean(episode_rewards)\n",
    "env_sd_reward = np.std(episode_rewards)\n",
    "# calculate number of wins\n",
    "total_reward = np.sum(episode_rewards)\n",
    "\n",
    "print()\n",
    "print(\"**************\")\n",
    "print(f\"Baseline Mean Reward={env_mean_random_reward:.2f}+/-{env_sd_reward:.2f}\", end=\"\")\n",
    "print()\n",
    "print(f\"Baseline won {total_reward} times over {num_episodes} episodes ({num_timesteps} timesteps)\")\n",
    "print(f\"Approx {total_reward/num_episodes:.2f} wins per episode\")\n",
    "print(\"**************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7328744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 17:21:57,603\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# configure and build the algorithm\n",
    "config = (PPOConfig()\n",
    "          .environment(\"CartPole-v1\")\n",
    "          .rollouts(num_rollout_workers=2)\n",
    "          .evaluation(evaluation_interval=15, evaluation_duration=5, evaluation_num_workers=1)\n",
    ")\n",
    "algo = config.build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba4d41b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-06-09 17:24:17</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:13.29        </td></tr>\n",
       "<tr><td>Memory:      </td><td>9.4/15.4 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/6.49 GiB heap, 0.0/3.25 GiB objects\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d35b7_00000</td><td>TERMINATED</td><td>192.168.1.69:20713</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         121.493</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">  451.82</td><td style=\"text-align: right;\">                 500</td><td style=\"text-align: right;\">                 119</td><td style=\"text-align: right;\">            451.82</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=20713)\u001b[0m 2023-06-09 17:22:07,288\tWARNING algorithm_config.py:488 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(PPO pid=20713)\u001b[0m 2023-06-09 17:22:07,288\tINFO algorithm_config.py:2503 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=20713)\u001b[0m 2023-06-09 17:22:07,425\tINFO algorithm.py:501 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPO pid=20713)\u001b[0m 2023-06-09 17:22:15,350\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>counters                                                                                                                        </th><th>custom_metrics  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                          </th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                      </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      </th><th>timers                                                                                                                                                                                   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_d35b7_00000</td><td style=\"text-align: right;\">                   4000</td><td>{&#x27;num_env_steps_sampled&#x27;: 4000, &#x27;num_env_steps_trained&#x27;: 4000, &#x27;num_agent_steps_sampled&#x27;: 4000, &#x27;num_agent_steps_trained&#x27;: 4000}</td><td>{}              </td><td style=\"text-align: right;\">           22.3333</td><td>{}             </td><td style=\"text-align: right;\">                  60</td><td style=\"text-align: right;\">              22.3333</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                 177</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_kl_coeff&#x27;: 0.20000000298023224, &#x27;cur_lr&#x27;: 4.999999873689376e-05, &#x27;total_loss&#x27;: 8.919505, &#x27;policy_loss&#x27;: -0.04005314, &#x27;vf_loss&#x27;: 8.954202, &#x27;vf_explained_var&#x27;: 0.0077485726, &#x27;kl&#x27;: 0.026782028, &#x27;entropy&#x27;: 0.66744447, &#x27;entropy_coeff&#x27;: 0.0, &#x27;model&#x27;: {}}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 128.0, &#x27;num_grad_updates_lifetime&#x27;: 465.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464.5}}, &#x27;num_env_steps_sampled&#x27;: 4000, &#x27;num_env_steps_trained&#x27;: 4000, &#x27;num_agent_steps_sampled&#x27;: 4000, &#x27;num_agent_steps_trained&#x27;: 4000}</td><td style=\"text-align: right;\">                     4000</td><td style=\"text-align: right;\">                     4000</td><td style=\"text-align: right;\">                   4000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                   4000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    2</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: 46.03333333333333, &#x27;ram_util_percent&#x27;: 61.21666666666667}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.2436084438090651, &#x27;mean_inference_ms&#x27;: 0.49566054389481057, &#x27;mean_action_processing_ms&#x27;: 0.05542348405026465, &#x27;mean_env_wait_ms&#x27;: 0.04498101053013429, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 60.0, &#x27;episode_reward_min&#x27;: 8.0, &#x27;episode_reward_mean&#x27;: 22.333333333333332, &#x27;episode_len_mean&#x27;: 22.333333333333332, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 177, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [24.0, 21.0, 51.0, 21.0, 30.0, 10.0, 17.0, 15.0, 27.0, 22.0, 27.0, 15.0, 34.0, 43.0, 9.0, 45.0, 12.0, 17.0, 20.0, 12.0, 31.0, 19.0, 28.0, 26.0, 26.0, 33.0, 30.0, 12.0, 50.0, 14.0, 10.0, 17.0, 45.0, 30.0, 38.0, 21.0, 25.0, 8.0, 15.0, 23.0, 25.0, 23.0, 13.0, 11.0, 16.0, 14.0, 25.0, 17.0, 11.0, 12.0, 18.0, 19.0, 17.0, 15.0, 16.0, 14.0, 41.0, 23.0, 15.0, 46.0, 26.0, 20.0, 21.0, 19.0, 26.0, 17.0, 17.0, 16.0, 20.0, 15.0, 17.0, 23.0, 20.0, 18.0, 25.0, 17.0, 29.0, 17.0, 24.0, 11.0, 13.0, 21.0, 15.0, 10.0, 21.0, 23.0, 22.0, 19.0, 10.0, 17.0, 13.0, 21.0, 32.0, 37.0, 22.0, 24.0, 14.0, 35.0, 9.0, 22.0, 9.0, 15.0, 28.0, 16.0, 55.0, 11.0, 12.0, 33.0, 31.0, 60.0, 27.0, 12.0, 33.0, 10.0, 32.0, 11.0, 18.0, 10.0, 13.0, 15.0, 23.0, 44.0, 36.0, 15.0, 13.0, 30.0, 41.0, 36.0, 23.0, 24.0, 15.0, 12.0, 12.0, 18.0, 10.0, 27.0, 15.0, 20.0, 17.0, 55.0, 13.0, 17.0, 29.0, 25.0, 30.0, 18.0, 14.0, 10.0, 24.0, 14.0, 25.0, 16.0, 19.0, 12.0, 14.0, 18.0, 54.0, 46.0, 12.0, 17.0, 17.0, 30.0, 31.0, 24.0, 26.0, 17.0, 18.0, 35.0, 12.0, 16.0, 30.0, 13.0, 33.0, 21.0, 25.0, 25.0, 54.0], &#x27;episode_lengths&#x27;: [24, 21, 51, 21, 30, 10, 17, 15, 27, 22, 27, 15, 34, 43, 9, 45, 12, 17, 20, 12, 31, 19, 28, 26, 26, 33, 30, 12, 50, 14, 10, 17, 45, 30, 38, 21, 25, 8, 15, 23, 25, 23, 13, 11, 16, 14, 25, 17, 11, 12, 18, 19, 17, 15, 16, 14, 41, 23, 15, 46, 26, 20, 21, 19, 26, 17, 17, 16, 20, 15, 17, 23, 20, 18, 25, 17, 29, 17, 24, 11, 13, 21, 15, 10, 21, 23, 22, 19, 10, 17, 13, 21, 32, 37, 22, 24, 14, 35, 9, 22, 9, 15, 28, 16, 55, 11, 12, 33, 31, 60, 27, 12, 33, 10, 32, 11, 18, 10, 13, 15, 23, 44, 36, 15, 13, 30, 41, 36, 23, 24, 15, 12, 12, 18, 10, 27, 15, 20, 17, 55, 13, 17, 29, 25, 30, 18, 14, 10, 24, 14, 25, 16, 19, 12, 14, 18, 54, 46, 12, 17, 17, 30, 31, 24, 26, 17, 18, 35, 12, 16, 30, 13, 33, 21, 25, 25, 54]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.2436084438090651, &#x27;mean_inference_ms&#x27;: 0.49566054389481057, &#x27;mean_action_processing_ms&#x27;: 0.05542348405026465, &#x27;mean_env_wait_ms&#x27;: 0.04498101053013429, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0}</td><td>{&#x27;training_iteration_time_ms&#x27;: 3631.338, &#x27;load_time_ms&#x27;: 0.215, &#x27;load_throughput&#x27;: 18641351.111, &#x27;learn_time_ms&#x27;: 1904.863, &#x27;learn_throughput&#x27;: 2099.889, &#x27;synch_weights_time_ms&#x27;: 3.206}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 17:24:17,895\tINFO tune.py:762 -- Total run time: 133.77 seconds (133.28 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# train the agent using Tuner\n",
    "stop_criteria = dict(time_total_s = 120) # trian for 2 min\n",
    "tuner = tune.Tuner(\n",
    "    config.algo_class,\n",
    "    param_space = config.to_dict(),\n",
    "    run_config = air.RunConfig(\n",
    "        local_dir = \"cartpole_logs\",\n",
    "        stop = stop_criteria,\n",
    "        verbose = 2\n",
    "    )\n",
    ")\n",
    "experiment_results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1f8db32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>config/lr</th>\n",
       "      <th>config/gamma</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>done</th>\n",
       "      <th>time_total_s</th>\n",
       "      <th>timers/training_iteration_time_ms</th>\n",
       "      <th>num_rollout_workers</th>\n",
       "      <th>evaluation_num_workers</th>\n",
       "      <th>num_envs_per_eval_worker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6665276e1ebb45938385226891a89d1f</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.99</td>\n",
       "      <td>451.82</td>\n",
       "      <td>451.82</td>\n",
       "      <td>140000</td>\n",
       "      <td>35</td>\n",
       "      <td>True</td>\n",
       "      <td>121.492769</td>\n",
       "      <td>3339.97</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      experiment_id  config/lr  config/gamma   \n",
       "0  6665276e1ebb45938385226891a89d1f    0.00005          0.99  \\\n",
       "\n",
       "   episode_reward_mean  episode_len_mean  timesteps_total  training_iteration   \n",
       "0               451.82            451.82           140000                  35  \\\n",
       "\n",
       "   done  time_total_s  timers/training_iteration_time_ms  num_rollout_workers   \n",
       "0  True    121.492769                            3339.97                    2  \\\n",
       "\n",
       "   evaluation_num_workers  num_envs_per_eval_worker  \n",
       "0                       1                         1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# examine the Ray Tune experiment results, get the appropriate learning rate\n",
    "df = experiment_results.get_dataframe()\n",
    "\n",
    "temp_columns = [\n",
    "    \"experiment_id\", \"config/lr\", \"config/gamma\", \"episode_reward_mean\",\n",
    "    \"episode_len_mean\", \"timesteps_total\", \"training_iteration\", \n",
    "    \"done\", \"time_total_s\", \n",
    "    \"timers/training_iteration_time_ms\", \n",
    "    \"config/num_workers\", \"config/evaluation_num_workers\",\n",
    "    \"config/num_envs_per_worker\"\n",
    "]\n",
    "temp = df.loc[:, temp_columns].head()\n",
    "temp.rename(columns={'config/evaluation_config/num_workers':'num_train_workers'}, inplace=True)\n",
    "temp.rename(columns={'config/num_envs_per_worker':'num_envs_per_eval_worker'}, inplace=True)\n",
    "temp.rename(columns={'config/evaluation_num_workers':'evaluation_num_workers'}, inplace=True)\n",
    "temp.rename(columns={'config/num_workers':'num_rollout_workers'}, inplace=True)\n",
    "display(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a23cca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start fresh, restart Ray in case it is already running\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b88cdba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 10:10:22,424\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "2023-06-11 10:10:30,276\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# configure a new agent based on the tuning results obtained above\n",
    "config2 = (\n",
    "    PPOConfig()\n",
    "    .environment(env = \"CartPole-v1\")\n",
    "    .rollouts(num_rollout_workers=2)\n",
    "    .evaluation(evaluation_interval=15, evaluation_duration=5, evaluation_num_workers=1)\n",
    "    .training(lr=0.00005)\n",
    ")\n",
    "algo = config2.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09da9e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(eval_result)\\n# convert num_iterations to num_episodes\\nnum_episodes = len(eval_result[\"hist_stats\"][\"episode_lengths\"]) * num_iterations\\n# convert num_iterations to num_timesteps\\nnum_timesteps = sum(result[\"hist_stats\"][\"episode_lengths\"] * num_iterations)\\n# calculate number of wins\\nnum_wins = np.sum(result[\"hist_stats\"][\"episode_reward\"])\\n\\nprint(f\"PPO won {num_wins} times over {num_episodes} episodes ({num_timesteps} timesteps)\")\\nprint(f\"Approx {num_wins/num_episodes:.2f} wins per episode\")'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the new agent using RLlib.train() in a loop\n",
    "num_iterations = 10\n",
    "rewards = []\n",
    "checkpoint_dir = \"saved_runs/ppo/\"\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    result = algo.train()\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "\n",
    "    # save a checkpoint and evaluate the policy at the end of the training\n",
    "    if (i==num_iterations-1):\n",
    "        checkpoint_file = algo.save(checkpoint_dir)\n",
    "        eval_result = algo.evaluate()\n",
    "\n",
    "'''print(eval_result)\n",
    "# convert num_iterations to num_episodes\n",
    "num_episodes = len(eval_result[\"hist_stats\"][\"episode_lengths\"]) * num_iterations\n",
    "# convert num_iterations to num_timesteps\n",
    "num_timesteps = sum(result[\"hist_stats\"][\"episode_lengths\"] * num_iterations)\n",
    "# calculate number of wins\n",
    "num_wins = np.sum(result[\"hist_stats\"][\"episode_reward\"])\n",
    "\n",
    "print(f\"PPO won {num_wins} times over {num_episodes} episodes ({num_timesteps} timesteps)\")\n",
    "print(f\"Approx {num_wins/num_episodes:.2f} wins per episode\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a2c73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tensorboard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d568956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 10:10:40,319\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n",
      "2023-06-11 10:10:40,483\tINFO trainable.py:790 -- Restored on 192.168.1.69 from checkpoint: saved_runs/ppo/checkpoint_000010\n",
      "2023-06-11 10:10:40,484\tINFO trainable.py:799 -- Current state after restoring: {'_iteration': 10, '_timesteps_total': None, '_time_total': 30.973118782043457, '_episodes_total': 372}\n"
     ]
    }
   ],
   "source": [
    "# reload the policy from checkponit and run inference\n",
    "checkpoint = \"./saved_runs/ppo/checkpoint_000010\"\n",
    "#new_config = PPOConfig()\n",
    "algo = config2.build()\n",
    "algo.restore(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf72886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# play and render the game\n",
    "num_episodes = 50\n",
    "total_reward = 0\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while True:\n",
    "        action = algo.compute_single_action(observation=obs, explore=False)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        #print(f\"episode: {ep}\")\n",
    "        #print(f\"obs: {new_obs}, reward: {total_reward}, done: {done}\")\n",
    "        env.render()\n",
    "env.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
